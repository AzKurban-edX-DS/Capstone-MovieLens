ass_pred <- as.matrix(preds)
dim(ass_pred)
str(glm_preds)
class(glm_preds)
length(glm_preds)
ass_pred <- matrix(preds, nrow = 115, ncol = 4, dimnames = list(NULL, models))
length(models)
models <- c("glm", "gamLoess", "knn", "rf")
length(models)
ass_pred <- matrix(preds, nrow = 115, ncol = 4, dimnames = list(NULL, models))
dim(ass_pred)
head(ass_pred)
votes <- rowMeans(pred == 7)
head(votes)
votes <- rowMeans(pred == "M")
dim(votes)
head(votes)
votes <- rowMeans(ass_pred == "M")
str(votes)
head(votes)
length(votes)
y_hat <- ifelse(votes > 0.5, "M", "B")
y_hat
mean(y_hat == "M")
mean(y_hat == test_y)
str(brca$y)
head(brca$y)
length(brca$y)
x_centered <- sweep(brca$x, 2, colMeans(brca$x))
x_scaled <- sweep(x_centered, 2, colSds(brca$x), FUN = "/")
sd(x_scaled[,1])
# The median value can be found using the following code:
median(x_scaled[,1])
pca <- prcomp(x_scaled)
summary(pca)    # see PC1 Cumulative Proportion
data.frame(pca$x[,1:2], type = brca$y) %>%
ggplot(aes(PC1, PC2, color = type)) +
geom_point()
data.frame(type = brca$y, pca$x[,1:10]) %>%
gather(key = "PC", value = "value", -type) %>%
ggplot(aes(PC, value, fill = type)) +
geom_boxplot()
set.seed(1)
test_index <- createDataPartition(brca$y, times = 1, p = 0.2, list = FALSE)
test_x <- x_scaled[test_index,]
test_y <- brca$y[test_index]
train_x <- x_scaled[-test_index,]
train_y <- brca$y[-test_index]
mean(train_y == "B")
mean(test_y == "B")
set.seed(1)
train_glm <- train(train_x, train_y, method = "glm")
glm_preds <- predict(train_glm, test_x)
mean(glm_preds == test_y)
set.seed(5)
train_loess <- train(train_x, train_y, method = "gamLoess")
loess_preds <- predict(train_loess, test_x)
mean(loess_preds == test_y)
set.seed(7)
tuning <- data.frame(k = seq(3, 21, 2))
train_knn <- train(train_x, train_y,
method = "knn",
tuneGrid = tuning)
train_knn$bestTune
knn_preds <- predict(train_knn, test_x)
mean(knn_preds == test_y)
library(randomForest)
set.seed(9)
tuning <- data.frame(mtry = c(3, 5, 7, 9))
train_rf <- train(train_x, train_y,
method = "rf",
tuneGrid = tuning,
importance = TRUE)
train_rf$bestTune
rf_preds <- predict(train_rf, test_x)
mean(rf_preds == test_y)
varImp(train_rf)
models <- c("glm", "gamLoess", "knn", "rf")
length(models)
ass_pred <- matrix(preds, nrow = 115, ncol = 4, dimnames = list(NULL, models))
dim(ass_pred)
head(ass_pred)
votes <- rowMeans(ass_pred == "M")
str(votes)
head(votes)
length(votes)
y_hat <- ifelse(votes > 0.5, "M", "B")
y_hat
mean(y_hat == test_y)
acc <- colMeans(ass_pred ==  test_y)
acc
?rowMeans
votes
votes2 <- rowSums(ass_pred == "M")
votes2
mean(asm_preds == test_y)
asm_preds <- ifelse(votes2 > 2, "M", "B")
mean(asm_preds == test_y)
y_hat <- ifelse(votes >= 0.5, "M", "B")
y_hat
mean(y_hat == test_y)
asm_preds <- ifelse(votes2 >= 2, "M", "B")
mean(asm_preds == test_y)
ensemble <- cbind(glm = glm_preds == "B",
loess = loess_preds == "B",
rf = rf_preds == "B",
knn = knn_preds == "B")
dim(ensemble)
?cbind
head(ensemble)
ensemble_preds <- ifelse(rowMeans(ensemble) > 0.5, "B", "M")
mean(ensemble_preds == test_y)
asm_preds2 <- ifelse(votes2 > 2, "B", "M")
mean(asm_preds2 == test_y)
votes_B <- rowSums(ass_pred == "B")
asm_preds2 <- ifelse(votes_B > 2, "B", "M")
mean(asm_preds2 == test_y)
mvotes_B <- rowMeans(ass_pred == "B")
asm_preds_B <- ifelse(mvotes_B >= 0.5, "B", "M")
#y_hat
mean(asm_preds_B == test_y)
asm_preds_B <- ifelse(mvotes_B > 0.5, "B", "M")
#y_hat
mean(asm_preds_B == test_y)
acc
head(ass_pred)
acc <- colMeans(ass_pred == test_y)
acc
models <- c("Logistic regression", "Loess", "K nearest neighbors", "Random forest", "Ensemble")
accuracy <- c(mean(glm_preds == test_y),
mean(loess_preds == test_y),
mean(knn_preds == test_y),
mean(rf_preds == test_y),
mean(ensemble_preds == test_y))
data.frame(Model = models, Accuracy = accuracy)
# Create edx and final_holdout_test sets
##########################################################
# Create edx and final_holdout_test sets
##########################################################
setwd("C:/git/edx/ds/Capstone-MovieLens/r/init")
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
options(timeout = 120)
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- "ml-10M100K.zip"
# if(!file.exists(dl))
#   download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
unzip(dl, ratings_file)
movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
unzip(dl, movies_file)
ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
mutate(userId = as.integer(userId),
movieId = as.integer(movieId),
rating = as.numeric(rating),
timestamp = as.integer(timestamp))
movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
mutate(movieId = as.integer(movieId))
movielens <- left_join(ratings, movies, by = "movieId")
# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
# set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>%
semi_join(edx, by = "movieId") %>%
semi_join(edx, by = "userId")
# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
library(caret)
library(lubridate)
library(dplyr)
# Prepare shared auxiliary functions -------------------------------------------
start_date <- function(){
print(date())
Sys.time()
}
end_date <- function(start){
print(date())
Sys.time() - start
}
rmse <- function(r) sqrt(mean(r^2))
RMSE <- function(true_ratings, predicted_ratings){
sqrt(mean((true_ratings - predicted_ratings)^2))
}
#-------------------------------------------------------------------------------
## The Netflix Prize Dataset
# https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/BigChaos.pdf
# Initial Data -----------------------------------------
np_training_set_cnt <- 100480507
np_probe_set_cnt <- 1408395  # subset of `training_set`
probe_set_ratio <- np_probe_set_cnt/np_training_set_cnt
#> [1] 0.0140166
np_qualifying_set_cnt <- 2817131
quiz_set_ratio <- 0.5
test_set_ratio <- 1 - quiz_set_ratio
np_rmse_accepted_max <- 0.8563
#-------------------------------------------------------
#> The goal of the contest is to predict the qualifying set (size: 2817131 samples)
#> and achieve a RMSE score of at least 0.8563 on the quiz subset,
#> to get qualifed for the Grand Prize.
#####################################################
# Inspired by:
# HarvardX: PH125.8x
# Data Science: Machine Learning, Section 6.2: Recommendation Systems
#> The proposed solution below uses information (including citates) from the folowing textbook:
#> "Introduction to Data Science" written by Rafael A. Irizarry:
#> https://rafalab.dfci.harvard.edu/dsbook-part-2/
# Reference: the Textbook section: 23.1 Case study: recommendation systems
# https://rafalab.dfci.harvard.edu/dsbook-part-2/highdim/regularization.html#sec-recommendation-systems
# Split the `edx` dataset in `train_set` & `probe_set` ------------------------
#str(edx)
# 'data.frame':	9000055 obs. of  6 variables:
# $ userId   : int  1 1 1 1 1 1 1 1 1 1 ...
# $ movieId  : int  122 185 292 316 329 355 356 362 364 370 ...
# $ rating   : num  5 5 5 5 5 5 5 5 5 5 ...
# $ timestamp: int  838985046 838983525 838983421 838983392 838983392 838984474 838983653 838984885 838983707 838984596 ...
# $ title    : chr  "Boomerang (1992)" "Net, The (1995)" "Outbreak (1995)" "Stargate (1994)" ...
# $ genres   : chr  "Comedy|Romance" "Action|Crime|Thriller" "Action|Drama|Sci-Fi|Thriller" "Action|Adventure|Sci-Fi" ...
#str(final_holdout_test)
# Prepare train & test datasets ----------------------------------------------
#> Let's see the number of unique users that provided ratings
#> and how many unique movies were rated:
edx |> summarize(n_distinct(userId), n_distinct(movieId))
#   n_distinct(userId) n_distinct(movieId)
# 1              69878               10677
#> Let's ignore the data for users who have not provided at least 100 ratings:
edx100 <- edx |>
group_by(userId) |>
filter(n() >= 100) |>
ungroup()
edx100 |> summarize(n_distinct(userId), n_distinct(movieId))
# `     n_distinct(userId)` `n_distinct(movieId)`
#                    <int>                 <int>
#   1                24115                 10665
#> For each one of these users, we will split their ratings into 80% for training
#> and 20% for testing:
set.seed(2006)
indexes <- split(1:nrow(edx100), edx100$userId)
test_ind <- sapply(indexes, function(i) sample(i, ceiling(length(i)*.2))) |>
unlist() |>
sort()
test_set <- edx100[test_ind,]
train_set <- edx100[-test_ind,]
#> To make sure we don’t include movies in the training set that should not be
#> there, we remove entries using the semi_join function:
test_set <- test_set |> semi_join(train_set, by = "movieId") |>
as.data.frame()
train_set <- mutate(train_set, userId = factor(userId), movieId = factor(movieId))
head(train_set)
#> Make sure userId and movieId in final hold-out test set
#> are also in `train_set` set
# probe_set <- probe_set_tmp |>
#   semi_join(train_set, by = "movieId") |>
#   semi_join(train_set, by = "userId")
#> We will use the array representation described in `Section 17.5`,
#> for the training data: we denote ranking for movie `j` by user `i`as `y[i,j]`.
#> To create this matrix, we use pivot_wider:
y <- select(train_set, movieId, userId, rating) |>
pivot_wider(names_from = movieId, values_from = rating) |>
column_to_rownames("userId") |>
as.matrix()
dim_y <- dim(y)
dim_y
#> [1] 24115 10626
movie_map <- train_set |> dplyr::select(movieId, title) |>
distinct(movieId, .keep_all = TRUE)
## First Model
# Reference: the Textbook section "23.3 A first model"
# https://rafalab.dfci.harvard.edu/dsbook-part-2/highdim/regularization.html#a-first-model
#>  A model that assumes the same rating for all movies and users with all
#>  the differences explained by random variation would look as follows:
# Y[i,j] = μ + ε[i,j]
# Naive RMSE -------------------------------------------------------
mu <- mean(train_set$rating)
mu
#> [1] 3.471931
# Model testing ----------------------------------------------------------------
naive_rmse <- RMSE(probe_set$rating, mu)
naive_rmse <- RMSE(test_set$rating, mu)
naive_rmse
#> [1] 1.062162
# str(final_holdout_test)
# head(final_holdout_test)
final_naive_rmse <- RMSE(final_holdout_test$rating, mu)
final_naive_rmse
#> [1] 1.061958
# ------------------------------------------------------------------------------
## User effects
# Reference: the Textbook section(new edition): 23.4 User effects
# https://rafalab.dfci.harvard.edu/dsbook-part-2/highdim/regularization.html#user-effects
# If we visualize the average rating for each user:
hist(rowMeans(y, na.rm = TRUE), nclass = 30)
# we notice that there is substantial variability across users.
#>  To account for this, we can use a linear model with a treatment effect `α[i]`
#>  for each user. The sum `μ + α[i]` can be interpreted as the typical
#>  rating user `i` gives to movies. We can write the model as:
# Y[i,j] = μ + α[i] + ε[i,j]
### Support functions ---------------------------------------------------------
#> Because we know ratings can’t be below 0.5 or above 5,
#> we define the function clamp:
clamp <- function(x, min = 0.5, max = 5) pmax(pmin(x, max), min)
# to keep predictions in that range and then compute the RMSE:
user_effects_rmse <- function(test_set, a){
test_set |>
left_join(data.frame(userId = as.integer(names(a)), a = a), by = "userId") |>
mutate(resid = rating - clamp(mu + a)) |>
filter(!is.na(resid)) |>
pull(resid) |> rmse()
}
### Model training ------------------------------------------------------
#min_user_rates <- 100
n_bins <- 30
train_set |>
group_by(userId) |>
summarize(user_ratings_avg = mean(rating)) |>
ggplot(aes(user_ratings_avg)) +
geom_histogram(bins = n_bins, color = "black")
#> We can show that the least squares estimate `α[i]` is just the average
#> of `y[i,j] - μ` for each user. So we can compute them this way:
a <- rowMeans(y - mu, na.rm = TRUE)
# Model testing ----------------------------------------------------------------
model_user_rmse <- user_effects_rmse(test_set, a)
model_user_rmse
#> [1] 0.9718791
final_model_user_rmse <- user_effects_rmse(final_holdout_test, a)
final_model_user_rmse
#> [1] 0.9720994
## Modeling movie effects
## Movie Effects
# Reference: the Textbook section 23.5 Movie effects
# https://rafalab.dfci.harvard.edu/dsbook-part-2/highdim/regularization.html#movie-effects
# Enhanced model ---------------------------------------------------------------
#> We know from experience that some movies are just generally rated higher
#> than others. We can use a linear model with a treatment effect `β[j]`
#> for each movie, which can be interpreted as movie effect or the difference
#> between the average ranking for movie `j` and the overall average `μ`:
# Y[i,j] = μ + α[i] + β[j] + ε[i,j]
#> We use an approximation by first computing the least square estimate `μ` and
#> α[i], and then estimating β[j] as the average of the residuals
#> `y[i,j] - μ - α[i]`:
### Support functions ---------------------------------------------------------
# to keep predictions in that range and then compute the RMSE:
user_and_movie_effects_rmse <- function(test_set, a, b){
test_set |>
left_join(data.frame(userId = as.integer(names(a)), a = a), by = "userId") |>
left_join(data.frame(movieId = as.integer(names(b)), b = b), by = "movieId") |>
mutate(resid = rating - clamp(mu + a + b)) |>
filter(!is.na(resid)) |>
pull(resid) |> rmse()
}
#-------------------------------------------------------------------------------
b <- colMeans(y - mu - a, na.rm = TRUE)
# Model testing ----------------------------------------------------------------
#> We can now construct predictors and see how much the `RMSE` improves:
model_user_movie_rmse <- user_and_movie_effects_rmse(test_set, a, b)
model_user_movie_rmse
#> [1] 0.8664145
final_model_user_movie_rmse <- user_and_movie_effects_rmse(final_holdout_test, a, b)
final_model_user_movie_rmse
n <- colSums(!is.na(y))
sums <- colSums(y - mu - a, na.rm = TRUE)
lambdas <- seq(0, 10, 0.1)
rmses <- sapply(lambdas, function(lambda){
b <-  sums / (n + lambda)
test_set |>
left_join(data.frame(userId = as.integer(names(a)), a = a), by = "userId") |>
left_join(data.frame(movieId = as.integer(names(b)), b = b), by = "movieId") |>
mutate(resid = rating - clamp(mu + a + b)) |> pull(resid) |> rmse()
})
head(rmses)
plot(lambdas, rmses, type = "l")
min_rmse <- min(rmses)
min_rmse
min(rmses)
min_lambda <- which.min(rmses)
min_lambda
min_lambda <- lambdas[which.min(rmses)]
min_lambda
b_reg <- sums / (n + lambda)
b_reg <- sums / (n + min_lambda)
head(b_reg)
reg_rmse <- function(test_set, b){
test_set |>
left_join(data.frame(userId = as.integer(names(a)), a = a), by = "userId") |>
left_join(data.frame(movieId = as.integer(names(b)), b = b), by = "movieId") |>
mutate(resid = rating - clamp(mu + a + b)) |> pull(resid) |> rmse()
}
rmses <- sapply(lambdas, function(lambda){
b <-  sums / (n + lambda)
reg_rmse(test_set, b)
# test_set |>
#   left_join(data.frame(userId = as.integer(names(a)), a = a), by = "userId") |>
#   left_join(data.frame(movieId = as.integer(names(b)), b = b), by = "movieId") |>
#   mutate(resid = rating - clamp(mu + a + b)) |> pull(resid) |> rmse()
})
# Here is a plot of the RMSE versus `λ`:
plot(lambdas, rmses, type = "l")
min(rmses)
#> [1] 0.8659219
min_lambda <- lambdas[which.min(rmses)]
min_lambda
n
reg_rmse(test_set, b_reg)
reg_rmse(final_holdout_test, b_reg)
reg_rmse <- function(test_set, b){
test_set |>
left_join(data.frame(userId = as.integer(names(a)), a = a), by = "userId") |>
left_join(data.frame(movieId = as.integer(names(b)), b = b), by = "movieId") |>
mutate(resid = rating - clamp(mu + a + b)) |>
filter(!is.na(resid)) |>
pull(resid) |> rmse()
}
rmses <- sapply(lambdas, function(lambda){
b <-  sums / (n + lambda)
reg_rmse(test_set, b)
})
# Here is a plot of the RMSE versus `λ`:
plot(lambdas, rmses, type = "l")
min(rmses)
#> [1] 0.8659219
min_lambda <- lambdas[which.min(rmses)]
min_lambda
reg_rmse(test_set, b_reg)
reg_rmse(final_holdout_test, b_reg)
r <- sweep(y - mu - a, 2, b_reg)
head(r_names)
r_names <- colnames(r)
head(r_names)
godfather_idx <- str_detect(r_names, "Godfather")
sum(godfather_idx)
gdfth_nms <- r_names[godfather_idx]
length(gdfth_nms)
dim(r)
movie_titles <- with(movie_map, title[match(colnames(r), movieId)])
str(movie_titles)
godfather_idx <- str_detect(movie_titles, "Godfather")
sum(godfather_idx)
gdfth_nms <- movie_titles[godfather_idx]
length(gdfth_nms)
gdfth_nms
library(gridExtra)
p12 <- qplot(r[ ,gdfth_nms[1]],
r[,gdfth_nms[2]],
xlab = gdfth_nms[1],
ylab = gdfth_nms[2])
p23 <- qplot(r[ ,gdfth_nms[2]],
r[,gdfth_nms[3]],
xlab = gdfth_nms[2],
ylab = gdfth_nms[3])
p13 <- qplot(r[ ,gdfth_nms[1]],
r[,gdfth_nms[3]],
xlab = gdfth_nms[1],
ylab = gdfth_nms[3])
grid.arrange(p12, p23 ,p13, ncol = 3)
gdfth_nms[1]
godfather_idx
which(godfather_idx)
length(gdfth_idx)
gdfth_idx <- which(godfather_idx)
length(gdfth_idx)
gdfth_idx
library(gridExtra)
p12 <- qplot(r[ ,gdfth_idx[1]],
r[,gdfth_idx[2]],
xlab = gdfth_idx[1],
ylab = gdfth_idx[2])
p23 <- qplot(r[ ,gdfth_idx[2]],
r[,gdfth_idx[3]],
xlab = gdfth_idx[2],
ylab = gdfth_idx[3])
p13 <- qplot(r[ ,gdfth_idx[1]],
r[,gdfth_idx[3]],
xlab = gdfth_idx[1],
ylab = gdfth_idx[3])
grid.arrange(p12, p23 ,p13, ncol = 3)
p14 <- qplot(r[ ,gdfth_idx[1]],
r[,gdfth_idx[4]],
xlab = gdfth_idx[1],
ylab = gdfth_idx[4])
p24 <- qplot(r[ ,gdfth_idx[2]],
r[,gdfth_idx[4]],
xlab = gdfth_idx[2],
ylab = gdfth_idx[4])
p34 <- qplot(r[ ,gdfth_idx[3]],
r[,gdfth_idx[4]],
xlab = gdfth_idx[3],
ylab = gdfth_idx[4])
grid.arrange(p14, p24 ,p34, ncol = 3)
r123 <- r[, c(gdfth_idx[1], gdfth_idx[2], gdfth_idx[3])]
str(r123)
cor(r123,
use="pairwise.complete") |>
knitr::kable()
grid.arrange(p12, p23 ,p13, ncol = 3)
