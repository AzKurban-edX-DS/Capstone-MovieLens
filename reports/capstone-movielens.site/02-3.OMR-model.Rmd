## Overall Mean Rating (Naive) Model
\

Let's begin our analysis by evaluating the simplest model described in [Section _23.3 The First Model_ of the *Course Textbook*](https://rafalab.dfci.harvard.edu/dsbook-part-2/highdim/regularization.html#a-first-model), and then gradually refine it through further research.
It is about a model that assumes the same rating for all movies and users with all the differences explained by random variation would look as follows:

$$
Y_{i,j} = \mu + \varepsilon_{i,j}
$$

with $\varepsilon_{i,j}$ independent errors sampled from the same distribution centered at 0 and $\mu$ the _true_ rating for all movies.

We know that the estimate that minimizes the RMSE is the least squares estimate of $\mu$ and, in this case, is the average of all ratings:

```{r pressure, echo=TRUE}
mu <- mean(edx$rating)
print(mu)
```

If we predict all unknown ratings with $\hat{\mu}$, we obtain the following RMSE: 
```{r}
mu.MSEs <- naive_model_MSEs(mu)
data.frame(fold_No = 1:5, MSE = mu.MSEs) |>
  data.plot(title = "MSE resuls of the 5-fold CV method applied to the Overall Mean Rating Model",
              xname = "fold_No", 
              yname = "MSE")

mu.RMSE <- sqrt(mean(mu.MSEs))
mu.RMSE
```
::: {.noteblock data-latex=""}
For the _Mean Squared Error_ data visualization we used [data.plot](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R#L592) function] defined in the [Data Visualization](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R#L447) section of the [data.helper.function.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R) script.
:::

```{r eval=FALSE}
data.plot <- function(data, 
                      title, 
                      xname, 
                      yname, 
                      xlabel = NULL, 
                      ylabel = NULL,
                      line_col = "blue",
                      # scale = 1,
                      normalize = FALSE) {
  y <- data[, yname]
  
  if (normalize) {
    y <- y - min(y)
  }
  
  if (is.null(xlabel)) {
    xlabel = xname
  }
  if (is.null(ylabel)) {
    ylabel = yname
  }
  
  aes_mapping <- aes(x = data[, xname], y = y)
  
  data |> 
    ggplot(mapping = aes_mapping) +
    ggtitle(title) +
    xlab(xlabel) +
    ylab(ylabel) +
    geom_point() + 
    geom_line(color=line_col)
}
```

Here we also used [naive_model_MSEs](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/common-helper.functions.R#L54) function defined in the [common-helper.functions.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/common-helper.functions.R) script (already mentioned above) to compute _Mean Squared Errors_ using _5-Fold Cross Validation_ method: 

```{r eval=FALSE }
naive_model_MSEs <- function(val) {
  sapply(edx_CV, function(cv_item){
    mse(cv_item$validation_set$rating - val)
  })
}
```

One more function, defined in the [same script](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/common-helper.functions.R), that we will need for further analysis of the current model, is the [naive_model_RMSE](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/common-helper.functions.R#L59) one:
```{r eval=FALSE }
naive_model_RMSE <- function(val){
  sqrt(mean(naive_model_MSEs(val)))
}
```
\newpage

### Ensure that `mu.RMSE` value is the best for the current model
\

If we plug in any other number, we will get a higher RMSE. Let's prove that by the following small investigation:
```{r }
  deviation <- seq(0, 6, 0.1) - 3

  deviation.RMSE <- sapply(deviation, function(delta){
    naive_model_RMSE(mu + delta)
  })
```

Let's make a quick investigation of the `deviation.RMSE` result we have just got: 
```{r}
data.frame(delta = deviation, 
           delta.RMSE = deviation.RMSE) |> 
data.plot(title = TeX(r'[RMSE as a function of deviation ($\delta$) from the Overall Mean Rating ($\hat{mu}$)]'),
              xname = "delta", 
              yname = "delta.RMSE", 
              xlabel = TeX(r'[$\delta$]'), 
              ylabel = "RMSE")

which_min_deviation <- deviation[which.min(deviation.RMSE)]
min_rmse = min(deviation.RMSE)

print_log1("Minimum RMSE is achieved when the deviation from the mean is: %1",
         which_min_deviation)

print_log1("Is the previously computed RMSE the best for the current model: %1",
         mu.RMSE == min_rmse)
```
```{r eval=FALSE}
RMSEs.ResultTibble.OMR <- RMSEs.ResultTibble |> 
  RMSEs.AddRow("Overall Mean Rating Model", mu.RMSE)
```
```{r}
RMSE_kable(RMSEs.ResultTibble.OMR)
```

To win the grand prize of $1,000,000, a participating team had to get an RMSE of at least 0.8563[@BigChaosSln]. So we can definitely do better![@IDS2_23-3]

\newpage
