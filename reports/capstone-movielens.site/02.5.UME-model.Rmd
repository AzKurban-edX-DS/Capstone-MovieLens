## User+Movie Effect (UME) Model
\

### Movie Effect Analysis
\

#### Movies' Popularity
\

::: {#textbook.23.5.movie_effects .sidebar }
In [23.5 Movie effects](https://rafalab.dfci.harvard.edu/dsbook-part-2/highdim/regularization.html#movie-effects) section of the *Course Textbook* the author draws our attention to the fact that some movies are generally rated higher than others. 
:::

To prove this fact, we can find out the movies with the highest number of ratings using the following code:
```{r, eval=FALSE}
edx.ordered_movie_ratings <- edx |> group_by(movieId, title) |>
  summarize(number_of_ratings = n()) |>
  arrange(desc(number_of_ratings))
```
```{r }
print(head(edx.ordered_movie_ratings))
```

\newpage

Now, we can figure out the most given ratings in order from most to least:
```{r, eval=FALSE}
edx.rating_groups <- edx |>  group_by(rating) |>
     summarise(count = n()) |>
     arrange(desc(count))
```
```{r, include=TRUE}
print(edx.rating_groups)
```

\newpage

#### Rating Distribution
\

The following code allows us to summarize that in general, half-star ratings are less common than whole-star ratings (e.g., there are fewer ratings of 3.5 than there are ratings of 3 or 4, etc.):
```{r, include=TRUE}
print(edx.rating_groups |> arrange(rating))
```

We can visually see that from the following plot:
```{r}
edx.rating_groups |>
  ggplot(aes(x = rating, y = count)) +
  geom_line() 

```

\newpage

The more sophisticated way of visualizing the rating distribution shown below is cited from the [Rating distribution plot](https://www.kaggle.com/code/amirmotefaker/movie-recommendation-system-using-r-best/notebook#Rating-distribution-plot) section of the article _Movie Recommendation System using R - BEST_[@MRS-R-BEST] already referenced above in the [Ratings per user] section of this report.

```{r}
edx.rating_groups |>
  ggplot(aes(x = rating, y = count)) +
  geom_bar(stat = "identity", fill = "#8888ff") +
  ggtitle("Rating Distribution") +
  xlab("Rating") +
  ylab("Occurrences Count") +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(n.breaks = 10) +
  theme_economist() +
  theme(axis.title.x = element_text(vjust = -5, face = "bold"), 
        axis.title.y = element_text(vjust = 10, face = "bold"), 
        plot.margin = margin(0.7, 0.5, 1, 1.2, "cm"))
```

> This graph is another confirmation of what we found out above: rounded ratings occur more often than half-stared ones. The upward trend previously discussed is now perfectly clear, although it seems to top right between the 3 and 4-star ratings lowering the occurrences count afterward. That might be due to users being more hesitant to rate with the highest mark for whichever reasons they might hold[@MRS-R-BEST].

The two more plots below, from the same article, give us an additional visual representation of the movies' popularity.

\newpage

##### [Ratings per movie plot](https://www.kaggle.com/code/amirmotefaker/movie-recommendation-system-using-r-best/notebook#Ratings-per-movie-plot)
\

```{r}
edx.movie_groups <- edx |>
  group_by(movieId) |>
  summarize(count = n())
  
edx.movie_groups |>
  ggplot(aes(x = movieId, y = count)) +
  geom_point(alpha = 0.2, color = "#4020dd") +
  geom_smooth(color = "red") +
  ggtitle("Ratings per movie") +
  xlab("Movies") +
  ylab("Number of ratings") +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(n.breaks = 10) +
  theme_economist() +
  theme(axis.title.x = element_text(vjust = -5, face = "bold"), 
        axis.title.y = element_text(vjust = 10, face = "bold"), 
        plot.margin = margin(0.7, 0.5, 1, 1.2, "cm"))
```
\newpage

##### [Movies' rating histogram](https://www.kaggle.com/code/amirmotefaker/movie-recommendation-system-using-r-best/notebook#Movies'-rating-histogram)
\

```{r}
edx.movie_groups |>
  ggplot(aes(x = count)) +
  geom_histogram(fill = "#8888ff", color = "#4020dd") +
  ggtitle("Movies' rating histogram") +
  xlab("Rating count") +
  ylab("Number of movies") +
  scale_y_continuous(labels = comma) +
  scale_x_log10(n.breaks = 10) +
  theme_economist() +
  theme(axis.title.x = element_text(vjust = -5, face = "bold"), 
        axis.title.y = element_text(vjust = 10, face = "bold"), 
        plot.margin = margin(0.7, 0.5, 1, 1.2, "cm"))
```

> This histogram better conveys the information provided by the summary() function, where the quantilesâ€™ values state that half the movies are rated between 30 and 565 times[@MRS-R-BEST].

\newpage


### Mathematical Description of the UME Model
\

The author of the *Course Textbook* mentioned [above](#textbook.23.5.movie_effects) also explains that in this case one can use a linear model with a _treatment effect_ $\beta_j$ for each movie, which can be interpreted as the movie effect, or the difference between the average rating for movie $j$ and the overall average $\mu$: 

$$
Y_{i,j} = \mu + \alpha_i + \beta_j +\varepsilon_{i,j}
$$
The author then shows how to use an approximation by first computing the least square estimate $\hat{\mu}$ and $\hat{\alpha}_i$, and then estimating $\hat{\beta}_j$ as the average of the residuals $y_{i,j} - \hat{\mu} - \hat{\alpha}_i$:

```{r eval=FALSE}
b <- colMeans(y - mu - a, na.rm = TRUE)
```

Inspired by this idea, a few support functions were developed by the author of this report, which we will use for our further analysis.

### UME Model: Support Functions
\

::: {.noteblock data-latex=""}
The complete source code of the functions described in this section is available in the [UME Model Support Functions](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/UM-effect.functions.R#L1) section of the [UM-effect.functions.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/UM-effect.functions.R) script on _GitHub_.
:::
 
#### [train_user_movie_effect](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/UM-effect.functions.R#L2) Function {#func.train_user_movie_effect}
\

We use this function to build and train our model using the `train_set` dataset:
```{r eval=FALSE}
train_user_movie_effect <- function(train_set, lambda = 0){
  if (is.na(lambda)) {
    stop("Function: train_user_movie_effect
`lambda` is `NA`")
  }

  UM.effect <- train_set |>
    left_join(edx.user_effect, by = "userId") |>
    mutate(resid = rating - (mu + a)) |> 
    group_by(movieId) |>
    summarise(b = mean_reg(resid, lambda), n = n())
  
  stopifnot(!is.na(mean(UM.effect$b)))
  UM.effect
}
```

::: {.noteblock data-latex=""}
The function described above accepts the `lambda` parameter, which we will need later for the _Model Regularization_ method. We also use the [mean_reg](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/common-helper.functions.R#L63) function call, which we will need later for the *Regularization* techniques (for details, see the  [mean_reg](#func.mean_reg) function description in the Section [Regularization: Common Helper Functions](#appndx_a.rglr.common_helper.functions) section of the [Appendix] to this report).
We will explain that later in the [User+Movie Effect Model Regularization] section. 
For now, we omit the `lambda` parameter, accepting its default value `lambda = 0`. In this case, the [mean_reg](#func.mean_reg) function is equivalent to the standard R function [base::mean](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/mean).
:::

#### `train_user_movie_effect.cv` Function {#func.train_user_movie_effect.cv}
\

We use the [train_user_movie_effect.cv](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/UM-effect.functions.R#L17) function to build and train our model using the `5-Fold Cross Validation` method. Below, we provide the most important part of the code of that function:
```{r eval=FALSE}
train_user_movie_effect.cv <- function(lambda = 0){
# ...
  start <- put_start_date()
  user_movie_effects_ls <- lapply(edx_CV, function(cv_fold_dat){
    cv_fold_dat$train_set |> train_user_movie_effect(lambda)
  })
  put_end_date(start)
  put_log("Function: train_user_movie_effect.cv:
User+Movie Effect list have been computed")
  
  user_movie_effects_united <- union_cv_results(user_movie_effects_ls)

  user_movie_effect <- user_movie_effects_united |>
    group_by(movieId) |>
    summarise(b = mean(b), n = mean(n))
  # ...
  user_movie_effect
}

```

::: {.noteblock data-latex=""}
Here we use the function call [union_cv_results](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R#L432), which is defined in the script [common-helper.functions.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R), to aggregate the _5-Fold Cross Validation_ method results (for details, see the [union_cv_results](#func.union_cv_results) function description in the [Data Helper Functions](#appndx_a.data_helper.functions) section of the [Appendix] to this report).
:::

#### `calc_user_movie_effect_MSE` Function
\

The source code of the function [calc_user_movie_effect_MSE](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/UM-effect.functions.R#L55) defined in the [UM-effect.functions.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/UM-effect.functions.R) script to calculate the _Mean Squared Error (MSE)_ of the _UME Model_ for the given `Test Set` is provided below:
```{r eval=FALSE}
calc_user_movie_effect_MSE <- function(test_set, um_effect){
  mse.result <- test_set |>
    left_join(edx.user_effect, by = "userId") |>
    left_join(um_effect, by = "movieId") |>
    mutate(resid = rating - clamp(mu + a + b)) |> 
    pull(resid) |> mse()
  
  stopifnot(!is.na(mse.result))
  mse.result
}
```

#### `calc_user_movie_effect_MSE.cv` Function
\

The source code of the function [calc_user_movie_effect_MSE.cv](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/UM-effect.functions.R#L72) defined in the [UM-effect.functions.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/UM-effect.functions.R) script to calculate the _5-Fold Cross Validation_ **MSE**_ result of the _UME Model_ is provided below:
```{r eval=FALSE}
calc_user_movie_effect_MSE.cv <- function(um_effect){
  put_log("Function: user_movie_effects_MSE.cv:
Computing the RMSE taking into account User+Movie Effects...")
  start <- put_start_date()
  user_movie_effects_MSEs <- sapply(edx_CV, function(cv_fold_dat){
    cv_fold_dat$validation_set |> calc_user_movie_effect_MSE(um_effect)
  })
  put_end_date(start)
  
  put_log1("Function: user_movie_effects_MSE.cv:
MSE values have been plotted for the %1-Fold Cross Validation samples.", 
           CVFolds_N)
  
  mean(user_movie_effects_MSEs)
}

```


#### `calc_user_movie_effect_RMSE` Function
\


The source code of the function [calc_user_movie_effect_RMSE](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/UM-effect.functions.R#L51) defined in the [UM-effect.functions.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/UM-effect.functions.R) script to calculate the `Root Mean Squared Error (RMSE)` of the `UME Model` for the given `Test Set` is provided below:

```{r eval=FALSE}
calc_user_movie_effect_RMSE <- function(test_set, um_effect){
  mse <- test_set |> calc_user_movie_effect_MSE(um_effect)
  sqrt(mse)
}
```

#### `calc_user_movie_effect_RMSE.cv` Function { #func.calc_user_movie_effect_RMSE.cv }
\

The source code of the function [calc_user_movie_effect_RMSE.cv](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/UM-effect.functions.R#L65) defined in the [UM-effect.functions.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/UM-effect.functions.R) script to calculate the `5-Fold Cross Validation` RMSE result of the `UME Model` is provided below:

```{r eval=FALSE}
calc_user_movie_effect_RMSE.cv <- function(um_effect){
  user_movie_effects_MSE <- calc_user_movie_effect_MSE.cv(um_effect)
  um_effect_RMSE <- sqrt(user_movie_effects_MSE)
  put_log2("Function: user_movie_effects_RMSE.cv:
%1-Fold Cross Validation ultimate RMSE: %2", CVFolds_N, um_effect_RMSE)
  um_effect_RMSE
}

```

\newpage

### UME Model Building
\

::: {.noteblock data-latex=""}
The complete source code of builing and training the current model is available in the [Model building: User+Movie Effect](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L721) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R) script on _GitHub_.
:::

Below, we provide the most significant part of the code for training our model using the `5-Fold Cross Validation` method:
```{r eval=FALSE}
  cv.UM_effect <- train_user_movie_effect.cv()
```
```{r}
str(cv.UM_effect)

par(cex = 0.7)
hist(cv.UM_effect$b, 30, xlab = TeX(r'[$\hat{beta}_{j}$)]'),
     main = TeX(r'[Histogram of $\hat{beta}_{j}$]'))
```


We can now construct predictors and see how much the `RMSE` improves[@IDS2_23-5]:
```{r eval=FALSE}
cv.UM_effect.RMSE <- calc_user_movie_effect_RMSE.cv(cv.UM_effect)

RMSEs.ResultTibble.UME <- RMSEs.ResultTibble.UE |> 
  RMSEs.AddRow("User+Movie Effect Model", cv.UM_effect.RMSE)

```
```{r}
RMSE_kable(RMSEs.ResultTibble.UME)
```

\newpage

### UME Model Regularization
\

[Section _23.6 Penalized least squares_ of the *Course Textbook*](https://rafalab.dfci.harvard.edu/dsbook-part-2/highdim/regularization.html#penalized-least-squares) explains why and how we should use _Penalized least squares_ to improve our predictions. The author also explains that the general idea of penalized regression is to control the total variability of the movie effects: 
$$
\sum_{j=1}^{n} \beta_j^2
$$ 
Specifically, instead of minimizing the least squares equation, we minimize an equation that adds a penalty:

\begin{equation}
\sum_{i,j} \left(y_{u,i} - \mu - \alpha_i - \beta_j \right)^2 + \lambda \sum_{j} \beta_j^2 
(\#eq:ME-penalty)
\end{equation}

The first term is just the sum of squares and the second is a penalty that gets larger when many $\beta_i$s are large. Using calculus, we can actually show that the values of $\beta_i$ that minimize this equation are:

\begin{equation}
\hat{\beta}_j(\lambda) = \frac{1}{\lambda + n_j} \sum_{i=1}^{n_i} \left(Y_{i,j} - \mu - \alpha_i\right)
(\#eq:ME-regularized)
\end{equation}

where $n_j$ is the number of ratings made for movie $j$. 

This approach will have our desired effect: when our sample size $n_j$ is very large, we obtain a stable estimate and the penalty $\lambda$ is effectively ignored since $n_j+\lambda \approx n_j$. Yet when the $n_j$ is small, then the estimate $\hat{\beta}_i(\lambda)$ is shrunken towards 0. The larger the $\lambda$, the more we shrink[@IDS2_23-6].

We will implement the _Regularization_ method on our  models (starting from the current model) in two steps:

  1. **Pre-configuration:** Preliminary determination of the optimal range of $\lambda$ values for the `5-Fold Cross Validation` samples;
  
  2. **Fine-tuning:** figuring out the value of $\lambda$ that minimizes the model's RMSE.
  
  3. **Retraining:** retraining the model with the best value of the parameter $\lambda$ obtained in the previous step.


\newpage

#### UME Model Regularization: Support Functions
\

::: {.noteblock data-latex=""}
The [regularize.test_lambda.UM_effect.cv]() function described below are defined in the [Regularization]() section of the [UM-effect.functions.R]() script.
:::

##### [regularize.test_lambda.UM_effect.cv](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/UM-effect.functions.R#L96) Function {#func.regularize.test_lambda.UM_effect.cv}
\

This function calculates _RMSE_ of the _UME Model_ using _5-Fold Cross Validation_ method for the given $\lambda$ parameter value:
```{r, eval=FALSE}
regularize.test_lambda.UM_effect.cv <- function(lambda){
  if (is.na(lambda)) {
    stop("Function: regularize.test_lambda.UM_effect.cv
`lambda` is `NA`")
  }
  um_effect <- train_user_movie_effect.cv(lambda)
  calc_user_movie_effect_RMSE.cv(um_effect)
}
```

::: {.noteblock data-latex=""}
Note that we reuse the function [train_user_movie_effect.cv](#func.train_user_movie_effect.cv) calling it from the [regularize.test_lambda.UM_effect.cv](#func.regularize.test_lambda.UM_effect.cv), but now with the $\lambda$ parameter different from the default ('lambda = 0') value.
:::

\newpage

#### UME Model Regularization: Pre-configuration
\

Let's perform the preconfiguration to determine the appropriate range of $\lambda$ for subsequent fine-tuning of our current model:

We are going to use the [tune.model_param](#func.tune.model_param) function described in the [Regularization: Common Helper Functions](#appndx_a.rglr.common_helper.functions) section of the [Appendix] to this report, passing the [regularize.test_lambda.UM_effect.cv](#func.regularize.test_lambda.UM_effect.cv) function as the value of the [fn_tune.test.param_value](#func.tune.model_param.params.fn_tune.test.param_value) parameter:

```{r, eval=FALSE}
lambdas <- seq(0, 1, 0.1)

cv.UME.preset.result <- 
  tune.model_param(lambdas, regularize.test_lambda.UM_effect.cv)

put_log1("Preliminary regularization set-up of `lambda`s range for the UME Model has been completed
for the %1-Fold Cross Validation samples.",
CVFolds_N)

```
```{r}
str(cv.UME.preset.result)
```

::: {.noteblock data-latex=""}
The complete version of the source code provided in this section can be found in the [UME Model Regularization: Pre-configuration](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L814) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R) script.
:::

Now, let's visualize the results of the $\lambda$ range preconfiguration:
``` {r}
cv.UME.preset.result$tuned.result |>
  data.plot(title = TeX(r'[UME Model Regularization: $\lambda$ Range Pre-configuration]'),
              xname = "parameter.value", 
              yname = "RMSE", 
              xlabel = TeX(r'[$\lambda$]'), 
              ylabel = "RMSE")
```

::: {.noteblock data-latex=""}
We use the custom data visualization function [data.plot](#func.data.plot) described in the [Data Helper Functions](#appndx_a.data_helper.functions) section of the [Appendix] to this report, which is defined in the [Data Visualization](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R#L447) section of the [data.helper.functions.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R#L592) script on _GitHub_.
:::

\newpage

#### UME Model Regularization: Fine-tuning
\

We are now ready to perform the fine-tuning step of our model _regularization_ process to determine the best value for the $\lambda$ parameter.

Here we are going to use the [model.tune.param_range](#func.model.tune.param_range) function described in the [Regularization: Common Helper Functions](#appndx_a.rglr.common_helper.functions) section of the [Appendix] to this report, passing the [regularize.test_lambda.UM_effect.cv](#func.regularize.test_lambda.UM_effect.cv) function as the value of the [fn_tune.test.param_value](#func.model.tune.param_range.params.fn_tune.test.param_value) parameter:
```{r, eval=FALSE}
endpoints <- 
  get_fine_tune.param.endpoints(cv.UME.preset.result$tuned.result)

UM_effect.loop_starter <- c(endpoints["start"], 
                            endpoints["end"], 
                            8)
UM_effect.loop_starter
#> [1] 0.3 0.5 8.0

UME.rglr.fine_tune.cache.base_name <- "UME.rglr.fine-tune"

UME.rglr.fine_tune.results <- 
  model.tune.param_range(UM_effect.loop_starter,
                         UME.rglr.fine_tune.cache.path,
                         UME.rglr.fine_tune.cache.base_name,
                         regularize.test_lambda.UM_effect.cv)
                         #endpoint.min_diff = 1e-07/4)

UME.rglr.fine_tune.RMSE.best <- UME.rglr.fine_tune.results$best_result["best_RMSE"]
```
```{r, echo=FALSE}
writeLines("*** Fine-tuning results object data structure ***")
str(UME.rglr.fine_tune.results)

writeLines("*** Fine-tuning: best results ***")
UME.rglr.fine_tune.results$best_result
```

::: {.noteblock data-latex=""}
The complete version of the source code provided in this section can be also found in the [Fine-tuning Step of the Regularization Method for the User+Movie Model](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L867) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R) script on _GitHub_.
:::

Let's visualize the fine-tuning results:
```{r}
UME.rglr.fine_tune.results$tuned.result |>
  data.plot(title = "UME Model Regularization: Fine-tuned result",
              xname = "parameter.value",
              yname = "RMSE",
              xlabel = TeX(r'[$\lambda$]'),
              ylabel = str_glue("Deviation from the best RMSE value (",
                                as.character(round(UME.rglr.fine_tune.RMSE.best, digits = 7)), 
                                ")"),
              normalize = TRUE)
```
\newpage

#### UME Model Regularization: Retraining Model with the best $\lambda$
\

Now, we can calculate the _Regularized User+Movie Effect_ by retraining our model on the entire `edx` dataset with the best value of the $\lambda$ parameter we just calculated, for the definitive _Root Mean Squared Error_ calculation and use in subsequent models.
```{r, eval=FALSE}
  UME.rglr.best_lambda <- best_result["param.best_value"]
  UME.rglr.best_lambda

  rglr.UM_effect <- train_user_movie_effect(edx, UME.rglr.best_lambda)
```
```{r, echo=FALSE}
writeLines("*** The Best Fine-tuning Results ***")
UME.rglr.fine_tune.results$best_result

writeLines("*** Regularized User+Movie Effect Structure ***")
str(rglr.UM_effect)

print_log1("Regularized User+Movie Effect Model has been re-trained for the best `lambda`: %1.",
         UME.rglr.best_lambda)
```

::: {.noteblock data-latex=""}
The complete version of the source code provided in this section are available in the [Re-training Regularized User+Movie Effect Model for the best $\lambda$](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L910) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L910) script on _GitHub_.
:::

We calculate the _Root Mean Squared Error_ for the ultimately computed _User+Movie Effect_ using [calc_user_movie_effect_RMSE.cv](#func.calc_user_movie_effect_RMSE.cv) function described above as follows:
```{r, eval=FALSE}
UME.rglr.retrain.RMSE <- calc_user_movie_effect_RMSE.cv(rglr.UM_effect)
```
```{r}
print_log1("The best RMSE after being regularized: %1",
         UME.rglr.retrain.RMSE)
```

Finally, we add the definitive result for the current model to our _Result Table_:
```{r, eval=FALSE}
RMSEs.ResultTibble.rglr.UME <- RMSEs.ResultTibble.UME |> 
  RMSEs.AddRow("Regularized User+Movie Effect Model", 
               UME.rglr.retrain.RMSE,
               comment = "Computed for `lambda` = %1" |>
                 msg.glue(UME.rglr.best_lambda))
```
```{r}
RMSE_kable(RMSEs.ResultTibble.rglr.UME)
```

\newpage

