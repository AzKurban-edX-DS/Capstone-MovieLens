## User+Movie Effect (UME) Model
\

::: {.noteblock data-latex=""}
The complete source code of the _User+Movie Effect_  (hereafter *UM Effect* or *UME* for short) computation described in this section is available in the [User+Movie Effect (UME) Model](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L382) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L382) script on _GitHub_.
:::

### Movie Effect Analysis
\

#### Movies' Popularity
\

::: {#textbook.24.1.movie_effects .sidebar }
In [Section 24.1 *Case study: recommendation systems / Movie effects*](https://rafalab.dfci.harvard.edu/dsbook-part-2/highdim/regularization.html#sec-movie-effects) of the *Course Textbook (New Edition)*, the author draws our attention to the fact that some movies are generally rated higher than others[@IDS2_24-1_ME]. 
:::

To prove this fact, we can find out the movies with the highest number of ratings using the following [code snippet](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L413):
```{r main-script@l413, eval=FALSE}
edx.ordered_movie_ratings <- edx |> group_by(movieId, title) |>
  summarize(number_of_ratings = n()) |>
  arrange(desc(number_of_ratings))
```
```{r}
head(edx.ordered_movie_ratings)
```

Now, we can figure out the most given ratings in order from most to least. The [code snippet](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L422) below performs this operation:
```{r main-script@l422, eval=FALSE}
edx.rating_groups <- edx |>  group_by(rating) |>
     summarise(count = n()) |>
     arrange(desc(count))
```
```{r}
print(edx.rating_groups)
```

\newpage

#### Rating Distribution
\

The following [line of code](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L430) allows us to summarize that in general, half-star ratings are less common than whole-star ratings (e.g., there are fewer ratings of 3.5 than there are ratings of 3 or 4, etc.):
```{r main-script@l430, include=TRUE}
print(edx |> group_by(rating) |> summarize(count = n()))
```

We can give even more visibility for the *rating distribution* by plotting the data for the *rating groups* (the `edx.rating_groups` object) we obtained above using the following [code snippet](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L433):

```{r main-script@l433}
edx.rating_groups |>
  ggplot(aes(x = rating, y = count)) +
  geom_line() 
```

\newpage

### Mathematical Description of the UME Model
\

In the source mentioned [above](#textbook.24.1.movie_effects) the author also explains that in this case one can use a linear model with a _treatment effect_ $\beta_j$ for each movie, which can be interpreted as the movie effect, or the difference between the average rating for movie $j$ and the overall average $\mu$[@IDS2_24-1]: 

$$
Y_{i,j} = \mu + \alpha_i + \beta_j +\varepsilon_{i,j}
$$
The author then shows how to use an approximation by first computing the least square estimate $\hat{\mu}$ and $\hat{\alpha}_i$, and then estimating $\hat{\beta}_j$ as the average of the residuals $y_{i,j} - \hat{\mu} - \hat{\alpha}_i$:

```{r eval=FALSE}
b <- colMeans(y - mu - a, na.rm = TRUE)
```

### UME Model Building
\

::: {.noteblock data-latex=""}
The complete source code of the _User+Movie (UM) Effect_ computation described in this section is available in the [UME Model Building](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L456) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L456) script on _GitHub_.
:::

Below is the [line of code](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L472) that trains our model with the use of *K-Fold Cross-Validation*, where the $K$ is the length of the [`edx_CV` Object] (described in detail in [Appendix B: Models Training Datasets]) to which the *K-Fold Cross-Validation* is applied (in *this Project* we use $K = 5$):
```{r main-script@l472, eval=FALSE}
  cv.UM_effect <- train_user_movie_effect.cv()
```

::: {.noteblock data-latex=""}
In the code snippet above we use the [train_user_movie_effect.cv](#func.train_user_movie_effect.cv) helper function described in the [Models Training: Support Functions]/[UME Model: Utility Functions] section of [Appendix A](#appndx_a).
:::

```{r}
str(cv.UM_effect)
```

The following [code snippet](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L507) plots a histogram that provides a visual representation of the variability of the *UM Effect* across movies in the *data frame object* we have just obtained:
```{r main-script@l507}
par(cex = 0.7)
hist(cv.UM_effect$b, 30, xlab = TeX(r'[$\hat{beta}_{j}$)]'),
     main = TeX(r'[Histogram of $\hat{beta}_{j}$]'))
```

We can now construct predictors and see how much the *RMSE* score improves[@IDS2_24-1_ME].

As we already outlined above, to accomplish such tasks, we use *K-Fold Cross-Validation*, where the $K$ is the length of the [`edx_CV` Object] (described in detail in [Appendix B: Models Training Datasets]).

::: {.noteblock data-latex=""}
In *this Project*, we use $K = 5$.
:::

The following [line of code](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L513) validates our model by computing the *RMSE* score for the predictors constructed based on the *UM Effect*:
```{r main-script@l513, eval=FALSE}
cv.UM_effect.RMSE <- calc_user_movie_effect_RMSE.cv(cv.UM_effect)
```
```{r}
cv.UM_effect.RMSE
```

::: {.noteblock data-latex=""}
In the code snippet above, we use the [calc_user_movie_effect_RMSE.cv](#func.calc_user_movie_effect_RMSE.cv) function described in the [Models Training: Support Functions]/[UME Model: Utility Functions] section of [Appendix A](#appndx_a). 
:::

\newpage

Finally, we add the *RMSE* value obtained above to our *Result Table* and print the table using the following [code snippet](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L516):
```{r main-script@l516, eval=FALSE}
RMSEs.ResultTibble.UME <- RMSEs.ResultTibble.UE |> 
  RMSEs.AddRow("UME Model", 
               cv.UM_effect.RMSE,
               comment = "User+Movie Effect (UME) Model")
```
```{r}
RMSE_kable(RMSEs.ResultTibble.UME)
```

::: {.noteblock data-latex=""}
In the code snippet above, we use the [RMSEs.AddRow](#func.RMSEs.AddRow) and [RMSE_kable](#func.RMSE_kable) functions described in the [Common Helper Functions]/[Result RMSEs Tibble Functions] section of [Appendix A](#appndx_a).
:::

\newpage

### UME Model Regularization
\

::: {.noteblock data-latex=""}
The complete version of the source code provided in this section can be found in the [UME Model Regularization](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L526) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L526) script.
:::

We begin this section with the concept's mathematical description presented below in the subsection [UME Model Regularization: *Mathematical Description*].

Next, we will implement the *UME Model Regularization* in the following three steps:

  1. **Pre-configuration:** (Described in subsection [UME Model Regularization: *Pre-configuration*]) Preliminary determination of the optimal range of *regularization parameter* $\lambda$ values for the *K-Fold Cross-Validation* samples, where the $K$ is the length of the [`edx_CV` Object] (described in detail in [Appendix B: Models Training Datasets]) to which the *K-Fold Cross-Validation* is applied (in *this Project* we use $K = 5$);
  
  2. **Fine-tuning:** (Described in the subsection [UME Model Regularization: *Fine-tuning*]) Determining the best value of $\lambda$ with the highest possible accuracy that minimizes the *RMSE* score for the model.

  3. **Retraining:** (Described in subsection [UME Model Regularization: *Retraining the Model with the best* $\lambda$]) Retraining the model with the best value of $\lambda$ determined in the previous step.

#### UME Model Regularization: *Mathematical Description*
\

[Section 24.2 *Penalized least squares*](https://rafalab.dfci.harvard.edu/dsbook-part-2/highdim/regularization.html#penalized-least-squares) of the _Course Textbook (New Edition)_ explains why and how we should use _Penalized least squares_ to improve our predictions. The author also explains that the general idea of penalized regression is to control the total variability of the movie effects[@IDS2_24-2]: 
$$
\sum_{j=1}^{n} \beta_j^2
$$ 
Specifically, instead of minimizing the least squares equation, we minimize an equation that adds a penalty:

\begin{equation}
\sum_{i,j} \left(y_{u,i} - \mu - \alpha_i - \beta_j \right)^2 + \lambda \sum_{j} \beta_j^2 
(\#eq:ME-penalty)
\end{equation}

The first term is just the sum of squares and the second is a penalty that gets larger when many $\beta_i$s are large. Using calculus, we can actually show that the values of $\beta_i$ that minimize this equation are:

\begin{equation}
\hat{\beta}_j(\lambda) = \frac{1}{\lambda + n_j} \sum_{i=1}^{n_j} \left(Y_{i,j} - \mu - \alpha_i\right)
(\#eq:ME-regularized)
\end{equation}

where $n_j$ is the number of ratings made for movie $j$. 

This approach will have our desired effect: when our sample size $n_j$ is very large, we obtain a stable estimate and the penalty $\lambda$ is effectively ignored since $n_j+\lambda \approx n_j$. Yet when the $n_j$ is small, then the estimate $\hat{\beta}_i(\lambda)$ is shrunken towards 0. The larger the $\lambda$, the more we shrink[@IDS2_24-1].

\newpage

#### UME Model Regularization: *Pre-configuration*
\

::: {.noteblock data-latex=""}
The complete version of the source code provided in this section can be found in the [UME Model Regularization: Pre-configuration](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L554) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L554) script.
:::

We need to do some pre-configuration to determine a suitable range of $\lambda$ for subsequent fine-tuning of our current model.
For this purpose, we will use the helper function [tune.model_param](#func.tune.model_param), passing the [regularize.test_lambda.UM_effect.cv](#func.regularize.test_lambda.UM_effect.cv) (another helper function) as the value of its argument [fn_tune.test.param_value](#func.tune.model_param.args.fn_tune.test.param_value).

::: {.noteblock data-latex=""}
The functions [tune.model_param](#func.tune.model_param) and [regularize.test_lambda.UM_effect.cv](#func.regularize.test_lambda.UM_effect.cv) are described in the sections [Common Helper Functions]/[Model Tuning Utils](#appndx_a.CHF.model_tuning_utils) and  [Models Training: Support Functions]/[UME Model: Regularization], respectively, of [Appendix A: Support Functions].
:::

The following [code snippet](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L571) performs this operation:
```{r main-script@l571, eval=FALSE}
lambdas <- seq(0, 1, 0.1)
cv.UME.preset.result <- 
  tune.model_param(lambdas, regularize.test_lambda.UM_effect.cv)

put_log1("Preliminary regularization set-up of `lambda`s range for the UME Model has been completed
for the %1-Fold Cross-Validation samples.",
CVFolds_N)
```
```{r}
str(cv.UME.preset.result)
cv.UME.preset.result$best_result
```

\newpage

Now, let's visualize the results of the $\lambda$ range pre-configuration using the following [code snippet](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L596):
``` {r main-script@l596}
cv.UME.preset.result$tuned.result |>
  data.plot(title = TeX(r'[UME Model Regularization: $\lambda$ Range Pre-configuration]'),
              xname = "parameter.value", 
              yname = "RMSE", 
              xlabel = TeX(r'[$\lambda$]'), 
              ylabel = "RMSE")
```

::: {.noteblock data-latex=""}
In the code snippet above, we use the custom data visualization function [data.plot](#func.data.plot) described in the [Data Helper Functions]/[Data Visualization Functions] section of [Appendix A](#appndx_a).
:::

\newpage

#### UME Model Regularization: *Fine-tuning*
\

::: {.noteblock data-latex=""}
The complete version of the source code provided in this section can be found in the [UME Model Regularization: Fine-tuning](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L607) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L607) script on _GitHub_.
:::

We are now ready to perform the *fine-tuning step* of our model _regularization_ process to determine the best value for the $\lambda$ parameter.

The following [piece of code](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L608) prepares the interval of values for the $\lambda$ parameter, over which the operation has to be done:
```{r main-script@l608, eval=FALSE}
endpoints <- 
  get_fine_tune.param.endpoints(cv.UME.preset.result$tuned.result)

UM_effect.loop_starter <- c(endpoints["start"], 
                            endpoints["end"], 
                            8)
```
```{r, echo=FALSE}
writeLines("*** Values of the endpoints and the divisor for the interval of `lambda` values ***")
```
```{r}
UM_effect.loop_starter
```

::: {.noteblock data-latex=""}
The helper function [get_fine_tune.param.endpoints](#func.get_fine_tune.param.endpoints) used in the code snippet above is described in the sections [Common Helper Functions]/[Model Tuning Utils](#appndx_a.CHF.model_tuning_utils) of [Appendix A: Support Functions].
:::

And the next [code snippet](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L617) below accomplishes the task of *fine-tuning* the model:
```{r main-script@l617, eval=FALSE}
UME.rglr.fine_tune.cache.base_name <- "UME.rglr.fine-tune"

UME.rglr.fine_tune.results <- 
  model.tune.param_range(UM_effect.loop_starter,
                         UME.rglr.fine_tune.cache.path,
                         UME.rglr.fine_tune.cache.base_name,
                         regularize.test_lambda.UM_effect.cv)

UME.rglr.fine_tune.RMSE.best <- UME.rglr.fine_tune.results$best_result["best_RMSE"]
```

::: {.noteblock data-latex=""}
The custom functions [model.tune.param_range](#func.model.tune.param_range) and [regularize.test_lambda.UM_effect.cv](#func.regularize.test_lambda.UM_effect.cv) used in the code snippet above are described in the sections [Common Helper Functions]/[Model Tuning Utils](#appndx_a.CHF.model_tuning_utils) and  [Models Training: Support Functions]/[UME Model: Regularization], respectively, of [Appendix A: Support Functions].
:::

\newpage

Below are the results of fine-tuning the *UME Model*:

```{r, echo=FALSE}
writeLines("*** Path to the cache directory for intermediate fine-tuning results ***")
```
```{r}
UME.rglr.fine_tune.cache.path
```
```{r, echo=FALSE}
writeLines("*** Fine-tuning results object data structure ***")
```
```{r}
str(UME.rglr.fine_tune.results)
```
```{r, echo=FALSE}
writeLines("*** Fine-tuning: best results ***")
```
```{r}
UME.rglr.fine_tune.results$best_result
UME.rglr.fine_tune.RMSE.best
```

\newpage

The following [code snippet](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L636) provides a visual representation of the *fine-tuning results* we have just computed:
```{r main-script@l636}
UME.rglr.fine_tune.results$tuned.result |>
  data.plot(title = "UME Model Regularization: Fine-tuned result",
              xname = "parameter.value",
              yname = "RMSE",
              xlabel = TeX(r'[$\lambda$]'),
              ylabel = str_glue("Deviation from the best RMSE value (",
                                as.character(round(UME.rglr.fine_tune.RMSE.best, digits = 7)), 
                                ")"),
              normalize = TRUE)
```

::: {.noteblock data-latex=""}
Note that in the code snippet above, we use the custom data visualization function [data.plot](#func.data.plot) (described in the [Data Helper Functions]/[Data Visualization Functions] section of [Appendix A](#appndx_a)) with the argument [normalize](#func.data.plot.args.normalize) set to `TRUE`, which means that deviations from the minimum $y$ value are used to plot, rather than the $y$ values themselves.
:::

\newpage 

#### UME Model Regularization: *Re-training on the `edx` with the best* $\lambda$
\

::: {.noteblock data-latex=""}
The complete version of the source code provided in this section are available in the [UME Model Regularization: Re-training with the best `lambda`](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L650) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L650) script on _GitHub_.
:::

Now, we can refine our _User+Movie Effect Model_ by retraining on the entire `edx` dataset with the best value of the $\lambda$ parameter we just figured out (let's call it *Regularized User+Movie Effect Model* or *Regularized UME Model* for short), for the definitive _RMSE_ calculation and use in subsequent models.

The following [code snippet](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L664) performs this operation:
```{r main-script@l664, eval=FALSE}
best_result <- UME.rglr.fine_tune.results$best_result
UME.rglr.best_lambda <- best_result["param.best_value"]

put_log1("Re-training Regularized User+Movie Effect Model for the best `lambda`: %1...",
         UME.rglr.best_lambda)

rglr.UM_effect <- train_user_movie_effect(edx, UME.rglr.best_lambda)
```

::: {.noteblock data-latex=""}
In the code snippet above we use the [train_user_movie_effect](#func.train_user_movie_effect) function described in the [Models Training: Support Functions]/[UME Model: Utility Functions] section of [Appendix A](#appndx_a).
:::

```{r, echo=FALSE}
writeLines("*** The Best UM Effect Fine-tuning Results ***")
```
```{r}
UME.rglr.fine_tune.results$best_result
```

```{r, echo=FALSE}
writeLines("*** Regularized UM Effect Structure ***")
```
```{r}
str(rglr.UM_effect)

print_log1("Regularized UME Model has been re-trained for the best `lambda`: %1.",
         UME.rglr.best_lambda)
```

\newpage

Now, we are ready to construct predictors and calculate the *RMSE* score for the ultimately *Regularized UME Model* using the following [line of code](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L712): 
```{r main-script@l712, eval=FALSE}
UME.rglr.retrain.RMSE <- calc_user_movie_effect_RMSE.cv(rglr.UM_effect)
```
```{r}
print_log1("The best RMSE for the UME Model after being regularized: %1",
         UME.rglr.retrain.RMSE)
```

::: {.noteblock data-latex=""}
In the code snippet above, we use the [calc_user_movie_effect_RMSE.cv](#func.calc_user_movie_effect_RMSE.cv) function described in the [Models Training: Support Functions]/[UME Model: Utility Functions] section of [Appendix A](#appndx_a). 
:::

Finally, we add the definitive *RMSE* value obtained above to our *Result Table* and print the table using the following [code snippet](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L720):
```{r main-script@l720, eval=FALSE}
RMSEs.ResultTibble.rglr.UME <- RMSEs.ResultTibble.UME |> 
  RMSEs.AddRow("Regularized User+Movie Effect Model", 
               UME.rglr.retrain.RMSE,
               comment = "Computed for `lambda` = %1" |>
                 msg.glue(UME.rglr.best_lambda))
```
```{r}
RMSE_kable(RMSEs.ResultTibble.rglr.UME)
```

::: {.noteblock data-latex=""}
In the code snippet above, we use the [RMSEs.AddRow](#func.RMSEs.AddRow) and [RMSE_kable](#func.RMSE_kable) functions described in the [Common Helper Functions]/[Result RMSEs Tibble Functions] section of [Appendix A](#appndx_a).
:::

\newpage

