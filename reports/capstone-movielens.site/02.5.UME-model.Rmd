## User+Movie Effect (UME) Model
\

### Movie Effect Analysis
\

#### Movies' Popularity
\

::: {#textbook.24.1.movie_effects .sidebar }
In [Section 24.1 *Case study: recommendation systems / Movie effects*](https://rafalab.dfci.harvard.edu/dsbook-part-2/highdim/regularization.html#movie-effects) of the *Course Textbook (New Edition)*, the author draws our attention to the fact that some movies are generally rated higher than others[@IDS2_24-1]. 
:::

To prove this fact, we can find out the movies with the highest number of ratings using the following code:
```{r, eval=FALSE}
edx.ordered_movie_ratings <- edx |> group_by(movieId, title) |>
  summarize(number_of_ratings = n()) |>
  arrange(desc(number_of_ratings))
```
```{r }
head(edx.ordered_movie_ratings)
```

Now, we can figure out the most given ratings in order from most to least:
```{r, eval=FALSE}
edx.rating_groups <- edx |>  group_by(rating) |>
     summarise(count = n()) |>
     arrange(desc(count))
```
```{r, include=TRUE}
edx.rating_groups
```

#### Rating Distribution
\

The following code allows us to summarize that in general, half-star ratings are less common than whole-star ratings (e.g., there are fewer ratings of 3.5 than there are ratings of 3 or 4, etc.):
```{r, include=TRUE}
edx.rating_groups |> arrange(rating)
```

We can visually see that from the following plot:
```{r}
edx.rating_groups |>
  ggplot(aes(x = rating, y = count)) +
  geom_line() 

```

\newpage

### Mathematical Description of the UME Model
\

In the source mentioned [above](#textbook.24.1.movie_effects) the author also explains that in this case one can use a linear model with a _treatment effect_ $\beta_j$ for each movie, which can be interpreted as the movie effect, or the difference between the average rating for movie $j$ and the overall average $\mu$[@IDS2_24-1]: 

$$
Y_{i,j} = \mu + \alpha_i + \beta_j +\varepsilon_{i,j}
$$
The author then shows how to use an approximation by first computing the least square estimate $\hat{\mu}$ and $\hat{\alpha}_i$, and then estimating $\hat{\beta}_j$ as the average of the residuals $y_{i,j} - \hat{\mu} - \hat{\alpha}_i$:

```{r eval=FALSE}
b <- colMeans(y - mu - a, na.rm = TRUE)
```

### UME Model Building
\

::: {.noteblock data-latex=""}
The complete source code of the _User+Movie Effect_ computation described in this section is available in the [User+Movie Effect (UME) Model](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L382) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L382) script on _GitHub_.
:::

Below, we provide the most significant part of the code for training our model with the use of *K-Fold Cross-Validation*, where the `K` is the length of the [`edx_CV` Object] (described in detail in [Appendix B: Models Training Datasets]) to which the *K-Fold Cross-Validation* is applied:
```{r eval=FALSE}
  cv.UM_effect <- train_user_movie_effect.cv()
```

::: {.noteblock data-latex=""}
In the code snippet above we use the [train_user_movie_effect.cv](#func.train_user_movie_effect.cv) function described in the *Models Training: Support Functions /* [UME Model: Utility Functions] section of [Appendix A: Support Functions].
:::

```{r}
str(cv.UM_effect)

par(cex = 0.7)
hist(cv.UM_effect$b, 30, xlab = TeX(r'[$\hat{beta}_{j}$)]'),
     main = TeX(r'[Histogram of $\hat{beta}_{j}$]'))
```


We can now construct predictors and see how much the `RMSE` improves[@IDS2_23-5]:
```{r eval=FALSE}
cv.UM_effect.RMSE <- calc_user_movie_effect_RMSE.cv(cv.UM_effect)

RMSEs.ResultTibble.UME <- RMSEs.ResultTibble.UE |> 
  RMSEs.AddRow("User+Movie Effect Model", cv.UM_effect.RMSE)

```
```{r}
RMSE_kable(RMSEs.ResultTibble.UME)
```

::: {.noteblock data-latex=""}
In the code snippet above, we use the following functions described in [Appendix A: Support Functions]: 

  - [calc_user_movie_effect_RMSE.cv](#func.calc_user_movie_effect_RMSE.cv) (described in the *Models Training: Support Functions /* [UME Model: Utility Functions] section) to calculate the *RMSE* for the *Current Model*;
  
  - [RMSEs.AddRow](#func.RMSEs.AddRow) and [RMSE_kable](#func.RMSE_kable) (described in the *Common Helper Functions /* [Result RMSEs Tibble Functions] section) to update and print the table above.
:::

\newpage

### UME Model Regularization
\

 [Section 24.1 *Case study: recommendation systems / Penalized Least Squares*](https://rafalab.dfci.harvard.edu/dsbook-part-2/highdim/regularization.html#penalized-least-squares) of the _Course Textbook (New Edition)_ explains why and how we should use _Penalized least squares_ to improve our predictions. The author also explains that the general idea of penalized regression is to control the total variability of the movie effects[@IDS2_24-1]: 
$$
\sum_{j=1}^{n} \beta_j^2
$$ 
Specifically, instead of minimizing the least squares equation, we minimize an equation that adds a penalty:

\begin{equation}
\sum_{i,j} \left(y_{u,i} - \mu - \alpha_i - \beta_j \right)^2 + \lambda \sum_{j} \beta_j^2 
(\#eq:ME-penalty)
\end{equation}

The first term is just the sum of squares and the second is a penalty that gets larger when many $\beta_i$s are large. Using calculus, we can actually show that the values of $\beta_i$ that minimize this equation are:

\begin{equation}
\hat{\beta}_j(\lambda) = \frac{1}{\lambda + n_j} \sum_{i=1}^{n_j} \left(Y_{i,j} - \mu - \alpha_i\right)
(\#eq:ME-regularized)
\end{equation}

where $n_j$ is the number of ratings made for movie $j$. 

This approach will have our desired effect: when our sample size $n_j$ is very large, we obtain a stable estimate and the penalty $\lambda$ is effectively ignored since $n_j+\lambda \approx n_j$. Yet when the $n_j$ is small, then the estimate $\hat{\beta}_i(\lambda)$ is shrunken towards 0. The larger the $\lambda$, the more we shrink[@IDS2_24-1].

We will implement the _Regularization_ method on our  models (starting from the current model) in following three steps:

  1. **Pre-configuration:** Preliminary determination of the optimal range of *regularization parameter* $\lambda$ values for the *K-Fold Cross-Validation* samples;
  
  2. **Fine-tuning:** Figuring out the best value of $\lambda$ with the highest possible accuracy that minimizes the *RMSE* of the model.

  3. **Retraining:** Retraining the model with the best value of $\lambda$ determined in the previous step.

\newpage

#### UME Model Regularization: *Pre-configuration*
\

We need to do some pre-configuration to determine a suitable range of $\lambda$ for subsequent fine-tuning of our current model.
For this purpose, we will use the helper function [tune.model_param](#func.tune.model_param), passing the [regularize.test_lambda.UM_effect.cv](#func.regularize.test_lambda.UM_effect.cv) (another helper function) as the value of its argument [fn_tune.test.param_value](#func.tune.model_param.args.fn_tune.test.param_value).

::: {.noteblock data-latex=""}
The functions [tune.model_param](#func.tune.model_param) and [regularize.test_lambda.UM_effect.cv](#func.regularize.test_lambda.UM_effect.cv) are described in the sections *Common Helper Functions /* [Regularization Functions](#appndx_a.CHF.regularization) and  *Models Training: Support Functions /* [UME Model: Regularization] of [Appendix A: Support Functions], respectively.
:::

Below we provide the most significant part of the code that performs this operation:
```{r, eval=FALSE}
lambdas <- seq(0, 1, 0.1)

cv.UME.preset.result <- 
  tune.model_param(lambdas, regularize.test_lambda.UM_effect.cv)

put_log1("Preliminary regularization set-up of `lambda`s range for the UME Model has been completed
for the %1-Fold Cross-Validation samples.",
CVFolds_N)
```
```{r}
str(cv.UME.preset.result)
cv.UME.preset.result$best_result
```

::: {.noteblock data-latex=""}
The complete version of the source code provided in this section can be found in the [UME Model Regularization: Pre-configuration](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L554) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L554) script.
:::

Now, let's visualize the results of the $\lambda$ range pre-configuration:
``` {r}
cv.UME.preset.result$tuned.result |>
  data.plot(title = TeX(r'[UME Model Regularization: $\lambda$ Range Pre-configuration]'),
              xname = "parameter.value", 
              yname = "RMSE", 
              xlabel = TeX(r'[$\lambda$]'), 
              ylabel = "RMSE")
```

::: {.noteblock data-latex=""}
In the code snippet above, we use the custom data visualization function [data.plot](#func.data.plot) described in the *Data Helper Functions /* [Data Visualization Functions] section of [Appendix A: Support Functions].
:::

\newpage

#### UME Model Regularization: Fine-tuning
\

We are now ready to perform the fine-tuning step of our model _regularization_ process to determine the best value for the $\lambda$ parameter.
For this purpose, we will use the helper function [model.tune.param_range](#func.model.tune.param_range), passing the [regularize.test_lambda.UM_effect.cv](#func.regularize.test_lambda.UM_effect.cv) function (already mentioned above in the [UME Model Regularization: *Pre-configuration*] section) as the value of its argument [fn_tune.test.param_value](#func.model.tune.param_range.args.fn_tune.test.param_value).

::: {.noteblock data-latex=""}
The function [model.tune.param_range](#func.model.tune.param_range) is described in the sections *Common Helper Functions /* [ Regularization Functions](#appndx_a.CHF.regularization) of [Appendix A: Support Functions].
:::

Below we provide the most significant part of the code that performs this operation:
```{r, eval=FALSE}
endpoints <- 
  get_fine_tune.param.endpoints(cv.UME.preset.result$tuned.result)

UM_effect.loop_starter <- c(endpoints["start"], 
                            endpoints["end"], 
                            8)
```

::: {.noteblock data-latex=""}
The helper function [get_fine_tune.param.endpoints](#func.get_fine_tune.param.endpoints) used in code snippet above is described in the sections *Common Helper Functions /* [ Regularization Functions](#appndx_a.CHF.regularization) of [Appendix A: Support Functions].
:::

```{r}
UM_effect.loop_starter
UME.rglr.fine_tune.cache.path
```
```{r, eval=FALSE}
UME.rglr.fine_tune.cache.base_name <- "UME.rglr.fine-tune"

UME.rglr.fine_tune.results <- 
  model.tune.param_range(UM_effect.loop_starter,
                         UME.rglr.fine_tune.cache.path,
                         UME.rglr.fine_tune.cache.base_name,
                         regularize.test_lambda.UM_effect.cv)

UME.rglr.fine_tune.RMSE.best <- UME.rglr.fine_tune.results$best_result["best_RMSE"]
```
```{r, echo=FALSE}
writeLines("*** Fine-tuning results object data structure ***")
```
```{r}
str(UME.rglr.fine_tune.results)
```
```{r, echo=FALSE}
writeLines("*** Fine-tuning: best results ***")
```
```{r}
UME.rglr.fine_tune.results$best_result
UME.rglr.fine_tune.RMSE.best
```

Let's visualize the fine-tuning results:
```{r}
UME.rglr.fine_tune.results$tuned.result |>
  data.plot(title = "UME Model Regularization: Fine-tuned result",
              xname = "parameter.value",
              yname = "RMSE",
              xlabel = TeX(r'[$\lambda$]'),
              ylabel = str_glue("Deviation from the best RMSE value (",
                                as.character(round(UME.rglr.fine_tune.RMSE.best, digits = 7)), 
                                ")"),
              normalize = TRUE)
```

::: {.noteblock data-latex=""}
The complete version of the source code provided in this section are available in the [Fine-tuning Step of the Regularization Method for the User+Movie Model](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L607) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L607) script on _GitHub_.
:::

\newpage

#### UME Model Regularization: Retraining Model with the best $\lambda$
\

Now, we can refine our _User+Movie Effect Model_ by retraining on the entire `edx` dataset with the best value of the $\lambda$ parameter we just figured out (let's call it *Regularized User+Movie Effect Model*), for the definitive _RMSE_ calculation and use in subsequent models.
```{r, eval=FALSE}
best_result <- UME.rglr.fine_tune.results$best_result
UME.rglr.best_lambda <- best_result["param.best_value"]

rglr.UM_effect <- train_user_movie_effect(edx, UME.rglr.best_lambda)
```

::: {.noteblock data-latex=""}
In the code snippet above we use the [train_user_movie_effect](#func.train_user_movie_effect) function described in the *Models Training: Support Functions /* [UME Model: Utility Functions] section of [Appendix A: Support Functions].

Later in this section, we will also use the custom logging function [print_log1](#func.print_log1) described in the [Logging Functions] section of *the same Appendix*.
:::

```{r}
```  

```{r, echo=FALSE}
writeLines("*** The Best Fine-tuning Results ***")
```
```{r}
UME.rglr.fine_tune.results$best_result
```

```{r, echo=FALSE}
writeLines("*** Regularized User+Movie Effect Structure ***")
```
```{r}
str(rglr.UM_effect)

print_log1("Regularized User+Movie Effect Model has been re-trained for the best `lambda`: %1.",
         UME.rglr.best_lambda)
```

::: {.noteblock data-latex=""}
The complete version of the source code provided in this section are available in the [Re-training Regularized Model for the best `lambda`](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L650) section of the [capstone-movielens.main.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/capstone-movielens.main.R#L650) script on _GitHub_.
:::

We calculate the *RMSE* for the ultimately *Regularized User+Movie Effect Model* using [calc_user_movie_effect_RMSE.cv](#func.calc_user_movie_effect_RMSE.cv) function (already used above in the [UME Model Building] section):
```{r, eval=FALSE}
UME.rglr.retrain.RMSE <- calc_user_movie_effect_RMSE.cv(rglr.UM_effect)
```
```{r}
print_log1("The best RMSE after being regularized: %1",
         UME.rglr.retrain.RMSE)
```

Finally, we add the definitive result for the current model to our _Result Table_:
```{r, eval=FALSE}
RMSEs.ResultTibble.rglr.UME <- RMSEs.ResultTibble.UME |> 
  RMSEs.AddRow("Regularized User+Movie Effect Model", 
               UME.rglr.retrain.RMSE,
               comment = "Computed for `lambda` = %1" |>
                 msg.glue(UME.rglr.best_lambda))
```
```{r}
RMSE_kable(RMSEs.ResultTibble.rglr.UME)
```

::: {.noteblock data-latex=""}
To print the table above, we use the [RMSEs.AddRow](#func.RMSEs.AddRow) and [RMSE_kable](#func.RMSE_kable) functions described in the [Result RMSEs Tibble Functions] section of [Appendix A: Support Functions].
:::

\newpage

