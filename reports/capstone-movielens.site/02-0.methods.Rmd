# Methods and Analysis
::: {.noteblock data-latex=""}
All the source code of the R-scripts is available on the project's [GitHub repository](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/tree/main/r)[@edx_capstone_movielens]. 
:::

## Defining Logging and Time Measuring Helper Functions
\

First, let's define some helper functions for logging and time-measuring features that we will use in our R scripts. Some of them are listed below:

```{r eval=FALSE }
# Logging Helper functions -----------------------------------------------------
msg.set_arg <- function(msg_template, arg, arg.name = "%1") {
  msg_template |> 
    str_replace_all(arg.name, as.character(arg))
}
msg.glue <- function(msg_template, arg, arg.name = "%1"){
  msg_template |>
    msg.set_arg(arg, arg.name) |>
    str_glue()
}

# ...
```


\newpage

## Preparing train and set datasets
\

We will split the `edx` dataset into a training set, which we will use to build and train our models, and a test set in which we will compute the accuracy of our predictions, the way described in [Section 23.1.1 *Movielens data*](https://rafalab.dfci.harvard.edu/dsbook-part-2/highdim/regularization.html#movielens-data) of the _Course Textbook_ mentioned above[@IDS2_23-1-1].We will also use the _5-Fold Cross Validation_ method as described in [Section 29.6 *Cross validation*](https://rafalab.dfci.harvard.edu/dsbook-part-2/ml/resampling-methods.html#cross-validation) of the _Course Textbook_[@IDS2_29-6]. 
To prepare datasets for processing, we will use the following functions, specifically designed for these operations:

  - [init_source_datasets](#func.init_source_datasets)

  - [make_source_datasets](#func.make_source_datasets)

::: {.noteblock data-latex=""}
The full source code of the function listed above is available in the [Initialize input datasets](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R#L86) section of the [data.helper.functions.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R) script on _GitHub_.
:::

### [init_source_datasets](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R#L209) Function {#func.init_source_datasets}

The _init_source_datasets_ function checks whether the required source datasets are cached on the hard disk, and, if so, loads the datasets from the cache file.

Otherwise, it calls the [make_source_datasets](#func.make_source_datasets) function, which creates all the necessary datasets.

Finally, it stores the data into a cache file on the disk. 

The simplified version of the function's source code is shown below:

```{r eval=FALSE}
init_source_datasets <- function(){
  put_log("Method `init_source_datasets`: 
Initializing sourse datasets...")
  
  if(file.exists(movielens_datasets_file_path)){
    movielens_datasets <- load_movielens_data_from_file(movielens_datasets_file_path)
  } else if(file.exists(movielens_datasets_zip)) {
    put_log("Method `init_source_datasets`: 
Unzipping MovieLens data file from zip-archive: {movielens_datasets_zip}...") 
    
    unzip(movielens_datasets_zip, movielens_datasets_file_path)
    
    if(!file.exists(movielens_datasets_file_path)) {
      put_log("Method `init_source_datasets`: 
File does not exists: {movielens_datasets_file}.")
      stop("Failed to unzip MovieLens data zip-archive.")
    }
    
    movielens_datasets <- load_movielens_data_from_file(movielens_datasets_file_path)
  } else {
    put_log("Method `init_source_datasets`: 
Creating datasets...")
    library(edx.capstone.movielens.data)
    put_log("Method `init_source_datasets`: 
Library attached: 'edx.capstone.movielens.data'")
    
    movielens_datasets <- make_source_datasets()
    put("Method `init_source_datasets`: 
All required datasets have been created.")
    
    put_log("Method `init_source_datasets`: 
Saving newly created input datasets to file...")
    save(movielens_datasets, file =  movielens_datasets_file_path)

    if(!file.exists(movielens_datasets_file_path)) {
      put_log("Method `init_source_datasets`: 
File was not created: {movielens_datasets_file}.")
      warning("MovieLens data was not saved to file.")
    } else {
      put_log("Method `init_source_datasets`: 
Datasets have been saved to file: {movielens_datasets_file_path}.") 

      put_log("Method `init_source_datasets`: 
Creating zip-archive: {movielens_datasets_zip}...") 
      zip(movielens_datasets_zip, movielens_datasets_file_path)
    # ...
    }
  }
  movielens_datasets
}
```

### [make_source_datasets](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R#L87) Function {#func.make_source_datasets}

Let's take a closer look at the objects we will receive as a result of executing this function.

```{r eval=FALSE}
make_source_datasets <- function(){
  # ...
  list(edx_CV = edx_CV,
       edx.mx = edx.mx,
       edx.sgr = edx.sgr,
       tuning_sets = tuning_sets,
       movie_map = movie_map,
       date_days_map = date_days_map)
}
```

#### `edx.mx` Matrix Object
\

We will use the array representation described in [Section 17.5 *Analysis of variance (ANOVA)*](https://rafalab.dfci.harvard.edu/dsbook-part-2/linear-models/treatment-effect-models.html#sec-anova) of the _Course Textbook_, for the training data[@IDS2_17-5]. For this purpose, we will convert the `edx` dataset into a matrix using the `tidyr::pivot_wider` function:

```{r eval=FALSE}

  put_log("Function: `make_source_datasets`: Creating Rating Matrix from `edx` dataset...")
  edx.mx <- edx |> 
    mutate(userId = factor(userId),
           movieId = factor(movieId)) |>
    select(movieId, userId, rating) |>
    pivot_wider(names_from = movieId, values_from = rating) |>
    column_to_rownames("userId") |>
    as.matrix()
  
  put_log("Function: `make_source_datasets`:
Matrix created: `edx.mx` of the following dimentions:")

```

```{r}
  str(edx.mx)
```

#### `edx.sgr` Object
\

To account for the Movie Genre Effect more accurately, we need a dataset with split rows for movies belonging to multiple genres:
```{r eval=FALSE}
  put_log("Function: `make_source_datasets`: 
To account for the Movie Genre Effect, we need a dataset with split rows 
for movies belonging to multiple genres.")
  edx.sgr <- splitGenreRows(edx)
```
```{r}
str(edx.sgr)
summary(edx.sgr)
```

Note that we use the `splitGenreRows` function to split rows of the original dataset:
```{r eval=FALSE}
splitGenreRows <- function(data){
  put("Splitting dataset rows related to multiple genres...")
  start <- put_start_date()
  gs_splitted <- data |>
    separate_rows(genres, sep = "\\|")
  put("Dataset rows related to multiple genres have been splitted to have single genre per row.")
  put_end_date(start)
  gs_splitted
}
```

::: {.noteblock data-latex=""}
The [source code](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R#L71) of the function mentioned above is also available in the [Data processing functions](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R#L1) section of the [data.helper.functions.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R) script on _GitHub_.
:::

#### `movie_map` Object
\

To be able to map movie IDs to titles we create the following lookup table:
```{r eval=FALSE}
movie_map <- edx |> select(movieId, title, genres) |> 
    distinct(movieId, .keep_all = TRUE)
  
  put_log("Function: `make_source_datasets`: Dataset created: movie_map")
```

```{r}
str(movie_map)
summary(movie_map)
```

Note that titles cannot be considered unique, so we cannot use them as IDs[@IDS2_23-1-1].


#### `date_days_map` Object
\

We have a `timestamp` field in the `edx` dataset.
To be able to map the date, year, and number of days since the earliest record in the `edx` dataset with the corresponding value in this field, we create the following lookup table:
```{r eval=FALSE}
  put_log("Function: `make_source_datasets`: Creating Date-Days Map dataset...")
  date_days_map <- edx |>
    mutate(date_time = as_datetime(timestamp)) |>
    mutate(date = as_date(date_time)) |>
    mutate(year = year(date_time)) |>
    mutate(days = as.integer(date - min(date))) |>
    select(timestamp, date_time, date, year, days) |>
    distinct(timestamp, .keep_all = TRUE)
  
  put_log("Function: `make_source_datasets`: Dataset created: date_days_map")
```
``` {r}
  str(date_days_map)
  summary(date_days_map)
```

#### `edx_CV` Object
\

Here we have a list of sample objects we need to perform the _5-Fold Cross Validation_ 
as explained in [Section 29.6.1 *K-fold cross validation*](https://rafalab.dfci.harvard.edu/dsbook-part-2/ml/resampling-methods.html#k-fold-cross-validation) of the _Course Textbook_[@IDS2_29-6-1]:
```{r eval=FALSE}
  start <- put_start_date()
  edx_CV <- lapply(kfold_index,  function(fold_i){
    
    put_log1("Method `make_source_datasets`: 
Creating K-Fold Cross Validation Datasets, Fold %1", fold_i)
    
    #> We split the initial datasets into training sets, which we will use to build 
    #> and train our models, and validation sets in which we will compute the accuracy 
    #> of our predictions, the way described in the `Section 23.1.1 Movielens data`
    #> (https://rafalab.dfci.harvard.edu/dsbook-part-2/highdim/regularization.html#movielens-data) 
    #> of the Course Textbook.
    
    split_sets <- edx |>
      sample_train_validation_sets(fold_i*1000)
    
    train_set <- split_sets$train_set
    validation_set <- split_sets$validation_set
    
    put_log("Function: `make_source_datasets`: 
Sampling 20% from the split-row version of the `edx` dataset...")
    split_sets.gs <- edx.sgr |>
      sample_train_validation_sets(fold_i*2000)

    train.sgr <- split_sets.gs$train_set
    validation.sgr <- split_sets.gs$validation_set
    
    # put_log("Function: `make_source_datasets`: Dataset created: validation.sgr")
    # put(summary(validation.sgr))
    
    #> We will use the array representation described in `Section 17.5 of the Textbook`
    #> (https://rafalab.dfci.harvard.edu/dsbook-part-2/linear-models/treatment-effect-models.html#sec-anova), 
    #> for the training data. 
    #> To create this matrix, we use `tidyr::pivot_wider` function:
    
    put_log("Function: `make_source_datasets`: Creating Rating Matrix from Train Set...")
    train_mx <- train_set |> 
      mutate(userId = factor(userId),
             movieId = factor(movieId)) |>
      select(movieId, userId, rating) |>
      pivot_wider(names_from = movieId, values_from = rating) |>
      column_to_rownames("userId") |>
      as.matrix()
    
    put_log("Function: `make_source_datasets`:
Matrix created: `train_mx` of the following dimentions:")
    put(dim(train_mx))


    list(train_set = train_set,
         train_mx = train_mx,
         train.sgr = train.sgr,
         validation_set = validation_set)
  })
  put_end_date(start)
  put_log("Function: `make_source_datasets`: 
Set of K-Fold Cross Validation datasets created: edx_CV")
```
```{r}
str(edx_CV)
```

::: {.noteblock data-latex=""}
This code snippet is a [part](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R#L140) of the [make_source_datasets](#func.make_source_datasets) _function_ code described above.
:::

Note that we used the [sample_train_validation_sets](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R#L15) function call to split the original dataset (`edx` in this case): 
```{r eval=FALSE}
    split_sets <- edx |>
      sample_train_validation_sets(fold_i*1000)
```

which returns a pair of train/validation sets:
```{r eval=FALSE}
sample_train_validation_sets <- function(data, seed){
  put_log("Function: `sample_train_validation_sets`: Sampling 20% of the `data` data...")
  set.seed(seed)
  validation_ind <- 
    sapply(splitByUser(data),
           function(i) sample(i, ceiling(length(i)*.2))) |> 
    unlist() |> 
    sort()
  
  put_log("Function: `sample_train_validation_sets`: 
Extracting 80% of the original `data` not used for the Validation Set, 
excluding data for users who provided no more than a specified number of ratings: {min_nratings}.")
  
  train_set <- data[-validation_ind,]
  
  put_log("Function: `sample_train_validation_sets`: Dataset created: train_set")
  put(summary(train_set))
  
  put_log("Function: `sample_train_validation_sets`: 
To make sure we donâ€™t include movies in the Training Set that should not be there, 
we exclude entries using the semi_join function from the Validation Set.")
  tmp.data <- data[validation_ind,]
  
  validation_set <- tmp.data |> 
    semi_join(train_set, by = "movieId") |> 
    semi_join(train_set, by = "userId") |>
    as.data.frame()
  
  # Add rows excluded from `validation_set` into `train_set`
  tmp.excluded <- anti_join(tmp.data, validation_set)
  train_set <- rbind(train_set, tmp.excluded)
  
  put_log("Function: `sample_train_validation_sets`: Dataset created: validation_set")
  put(summary(validation_set))

  # CV train & test sets Consistency Test
  validation.left_join.Nas <- train_set |>
    mutate(tst.col = rating) |>
    select(userId, movieId, tst.col) |>
    data.consistency.test(validation_set)
  
  put_log("Function: `sample_train_validation_sets`:
Below are the data consistency verification results")
  put(validation.left_join.Nas)
  
  # Return result datassets ----------------------------------------------------    
  list(train_set = train_set, 
       validation_set = validation_set)
}

```

::: {.noteblock data-latex=""}
The [sample_train_validation_sets](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/data.helper.functions.R#L15) function is defined in the same script as the [make_source_datasets](#func.make_source_datasets), from which it is called.
:::

\newpage

### Common Helper Functions
\

::: {.noteblock data-latex=""}
All the _Common Helper functions_ described below in this section are defined in the [common-helper.functions.R](https://github.com/AzKurban-edX-DS/Capstone-MovieLens/blob/main/r/src/support-functions/common-helper.functions.R) script on _GitHub_. 
:::

For our further analysis, we are going to use the following _common helper functions_:

#### Functions to calculate _(Root) Mean Squared Error_
\

We will need the following functions to calculate _(R)MSEs_:
```{r eval=FALSE}
mse <- function(r) mean(r^2)

mse_cv <- function(r_list) {
  mses <- sapply(r_list, mse(r))
  mean(mses)
}

rmse <- function(r) sqrt(mse(r))
# rmse_cv <- function(r_list) sqrt(mse_cv(r_list))

rmse2 <- function(true_ratings, predicted_ratings) {
  rmse(true_ratings - predicted_ratings)
}
```

\newpage

